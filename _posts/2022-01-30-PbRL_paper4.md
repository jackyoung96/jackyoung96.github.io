---
layout: post
title: "Paper review - Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback"
---

다섯 번째 논문 리뷰입니다. Preference-based RL 분야에서 활발하게 연구를 진행하고 있는 Berkeley Pieter Abbeel 연구진에서 나온, PEBBLE을 기반으로 하고 있는 새끼 논문 중 하나입니다. 2022년 PMRL에 발표된 따끈따끈한 논문이죠. 한줄 요약 하자면, RL 분야에서 Long-horizon 문제를 해결하기 위해 제안된 skill learning을 PbRL과 접목시킨 연구입니다.  

저자는 Skill Preference (SkiP) 이라는 알고리즘을 새롭게 제안했습니다.
> an algorithm that learns a model over human preferences and uses it to extract human-aligned skills from offline data

사실 PEBBLE을 아주 조금 skill learning에 맞게 변형한 것인데요. 역시 좋은 논문을 한편 쓰면 leverage 효과로 논문들이 쏟아져 나온다는 것을 보여주는 좋은 예시라고 생각합니다.

<br>

Table of Contents
   - [Introdunction](#introdunction)
     * [Skill learning이란?](#skill-learning---)
     * [Contribution](#contribution)
   - [Background](#background)
     * [Skill](#skill)
     * [Method](#method)
     * [Skill extraction : Learning behavior priors with human feedback](#skill-extraction---learning-behavior-priors-with-human-feedback)
     * [Skill execution : Reward learning and human preference over skills](#skill-execution---reward-learning-and-human-preference-over-skills)
   -  [Experiment setup](#experiment-setup)
   - [Experimental results](#experimental-results)
   - [Conclusion](#conclusion)

<br><br>

## Introdunction

PbRL은 reward engineering을 하지 않기 위해서 시작된 분야입니다. 하지만 기존 연구들은 아주 간단한 task들을 푸는데 그쳤는데요. 사실 간단한 task들은 reward shaping이 크게 어렵지 않기 때문에 PbRL의 효용이 크지 않다고 봅니다. 그래서 이 논문은 아래 질문을 해결하기 위해 시작되었습니다.

> How can we learn robotic control policies that are aligned with human intent and capable of solving complex real-world tasks?

Long-horizon 문제는 RL에서 오랫동안 다뤄져 온 문제입니다. 사람이 하루 종일 생활하면서 단 한개의 policy 만 사용한다고 생각한다면, 하루 동안 마주치는 모든 task를 해결할 수 있는 아주 거대하고 성능이 뛰어난 policy가 필요하겠죠. 하지만 그런 policy를 만들어 내는건 computation burden이 클 뿐더러, 엄청난 state space를 탐색해야만 하는 문제를 가지고 있습니다. 이를 해결하기 위해 가장 널리 연구되는 분야는 Skill learning 입니다.  

<br>

### Skill learning이란?

Skill 이라는 latent vector를 하나 두고, policy가 skill에 dependent 하도록 하는 겁니다. policy execution을 두 단계로 나눠서, 1) 지금 state에서는 어떤 skill을 사용할 것인가? 2) 이 skill과 state를 바탕으로 어떤 action을 뽑아낼 것인가? 로 policy를 정의하는 것이죠.  

더 간단하게 예시를 들어보겠습니다. 로봇 팔이 부엌에서 계란 후라이를 하는 task를 생각해 보죠. 이것을 skill들로 나누면, 1) 찬장을 열고 2) 후라이팬을 꺼내고 3) 계란을 깨고 4) 불을 켜고 5) 계란을 뒤집고 6) 불을 끄는 policy들로 나눌 수 있습니다. 각 skill들은 latent vector로 표현되기 때문에, 우리가 정확히는 어떤 skill인지는 알 수 없습니다.

보통 Skill은 expert demonstration으로 구성된 offline dataset에서 추출합니다. 왜냐하면 expert들은 skill을 가지고 행동을 취한 것일테니까, 여가에서 skill 추출이 가능할 것이라는 가정입니다. Skill은 일반적으로 latent vector를 추출하는 대표적인 방식인 autoencoder (AE)를 이용하여 추출됩니다.  

![images](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/29202749/Blog_info_29-04-2020-R-02-1024x522.png?style=centerme){:width="60%"}

위와 같은 AE 구조는 익숙하지만, RL에서 AE를 어떻게 사용할까요? Input으로는 state가 들어가고, output으로는 action이 나오는 구조입니다. 각 state에서 취했던 action을 복원하는 거죠. 이걸 state와 action sequence로 확장하면, 중간에 latent vector는 skill을 나타내게 될 것이라는게 주요 내용입니다.  

수식으로 나타내면 다음과 같습니다. 이 때 $$p_\alpha(\mathbf{a}_t\|s_t)$$ 는 generative model (AE)를 의미하고, $$\mathbf{a}_t=(a_t,\dots,a_{t+H-1}$$의 action sequence를 의미합니다. Action sequence를 잘 복원해내는 latent vector, 즉 skill을 찾는 것입니다.

<center>
$$p_{\alpha} \in \arg \max _{\alpha} \mathbb{E}_{\tau \sim \mathcal{D}}\left[\sum_{t=0} \log \left(p_{\alpha}\left(\mathbf{a}_{t} \mid s_{t}\right)\right)\right]$$
</center>

<br>

### Contribution

그렇다면 Human preference는 skill learning에 어떻게 사용될까요? 전체 모식도는 아래 그림과 같습니다.

![image](https://user-images.githubusercontent.com/57203764/151723906-242e0ca9-943e-4278-b549-8c059dbb5cb1.png?style=centerme){:width="70%"}

Human preference는 총 두 번 사용됩니다. Skill을 extraction 하는 곳에서 한 번, Skill execution 하는 downstream task를 학습하는 곳에서 한 번. 본 논문의 contribution은 다음과 같습니다.

- Skill learning과 Human preference를 결합하는 새로운 알고리즘을 제안하였다.
- Skill extraction에 Human preference를 사용해 noisy offline dataset에서도 강건하게 동작하였다.
- 실제 D4RL 환경에서 실험하여 성능을 확인하였다.

<br><br>

## Background

### Skill

아주 간단하게 skill을 수식으로 정의하고 넘어가도록 하겠습니다.   

- Leanring skills $$z\in\mathcal{Z}$$

- encoder : $$q^{(e)}\left(z \mid s_{t}, a_{t}, \ldots, s_{t+H-1}, a_{t+H-1}\right)$$ → skill extraction

- decoder : $$q^{(d)}\left(a_{1}, a_{2}, \ldots, a_{H} \mid s, z\right)$$ → action sequence reconstruction (학습에만 사용하고 불필요한 부분)

<br><br>

### Method

제안된 알고리즘 SkiP은 2 단계 (kill extraction, skill execution)로 나뉘어 있습니다. 전체 알고리즘은 아래와 같고, 각각에 대해서 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/57203764/151728233-e5345d8b-349c-4302-acae-e33384923c57.png?style=centerme){:width="70%"}


### Skill extraction : Learning behavior priors with human feedback

Skill extraction은 Introduction에서 언급했듯이, expert demo로 구성된 offline database로부터 AE를 이용해 skill을 추출합니다.
<center>
$$p_{\alpha} \in \arg \max _{\alpha} \mathbb{E}_{\tau \sim \mathcal{D}}\left[\sum_{t=0} \log \left(p_{\alpha}\left(\mathbf{a}_{t} \mid s_{t}\right)\right)\right]$$
</center>

AE를 이용한 skill extraction의 가장 취약한 점은 이상한 trajectory가 포함되어 있을 때 제대로 된 skill을 추출하지 못한다는 것입니다. skill과는 관계도 없고 task를 제대로 해결하지도 못하는 policy가 만들어낸 trajectory에 skill이 있을리가 없겠죠? 그렇다고해서 좋은 trajectory만을 database에 모으겠다는 것은 굉장히 노동력이 많이 들고, 비싼 작업입니다.  

그래서 제안된 방법은 **weighted behavioral priors** 라는 것입니다. 말 그대로 좋은 trajectory에 가중치를 둬서 skill을 추출하겠다는 이야기죠. 그냥 원래 식에다가 가중치만 곱해주면 됩니다.

<center>
$$p_{\alpha} \in \arg \max _{\alpha} \mathbb{E}_{\tau \sim \mathcal{D}}\left[\sum_{t=0}^{|\tau|} \exp \left(\omega\left(\tau_{t}\right) / T\right) \cdot \log \left(p_{\alpha}\left(\mathbf{a}_{t} \mid s_{t}\right)\right)\right]$$
</center>

그러면 가중치를 어떻게 아냐고요? 바로 여기서 Human preference를 쓰는 거죠. 바로 Human preference 가 높을 수록 가중치가 커질 수 있도록 함수를 만들어주는 겁니다.

<center>
Human preference predictor (Binary predictor)  
$$\omega\left(\tau_{t}\right):=\log P_{\psi}\left(\tau_{t}\right)$$
</center>

방법은 무식하지만 심플합니다. Database에서 무작위로 선택된 10%의 데이터를 사용해서 preference predictor를 학습합니다. Binary classification이니까 아주 간단하게 학습이 되겠습니다.  

AE의 경우 Variational AE를 사용합니다. 사실 VAE는 AE는 아닙니다. 구조가 비슷할 뿐이지만, latent vector를 추출한다는 의미 자체에서는 유사합니다. 무엇보다 Gaussian distribution 형태의 latent space를 추출한다는 점에서, 더 강력한 encoder를 학습시킬 수 있습니다.  
VAE의 경우 ELBO라고 불리우는 Bounded loss function을 이용하여 학습합니다.

<center>
$$\log p\left(\mathbf{a}_{t} \mid s_{t}\right) \geq \mathbb{E}_{\tau \sim \mathcal{D}, z \sim q_{\phi_{2}}(z \mid \tau)}[\underbrace{\log p_{\phi_{1}}\left(\mathbf{a}_{t} \mid s_{t}, z\right)}_{\mathcal{L}_{\text {rec }}}+\beta \underbrace{\left(\log p(z)-\log q_{\phi_{2}}(z \mid \tau)\right.}_{\mathcal{L}_{\text {reg }}}]$$
</center>

여기에 preference predictor weight를 곱해준 최종 Loss는 다음과 같습니다.

<center>
$$\mathcal{L}=\arg \max _{\phi_{1}, \phi_{2}} E_{\tau \sim \mathcal{D}, z \sim q_{\phi}(z \mid \tau)}\left[P_{\psi}(\tau)\left(\mathcal{L}_{\mathrm{rec}}+\mathcal{L}_{\mathrm{reg}}\right)\right]$$
</center>

<br>

### Skill execution : Reward learning and human preference over skills

Downstream task의 경우 기존의 RL과 완전히 같은 방식으로 학습됩니다. 물론 여기서는 그냥 RL이 아닌 PbRL을 사용하고, PEBBLE을 그대로 가져다가 씁니다 (논문 레버리징... 부럽습니다). PEBBLE과 다른 점은 trajectory의 길이 뿐입니다.

<center>
$$\tau^{(z)}=\left(s_{t}, z_{t}, s_{t+H}, z_{t+H}, \ldots, s_{(t+M) H}, z_{(t+M) H}\right)$$
</center>
여기에서 $$H$$는 skill을 한 번 선택한 뒤 action을 취하는 개수이고 $$M$$은 총 skill의 변화 횟수입니다.  
이거는 skill learning 고수 친구에게 물어본 것인데, skill을 언제 바꿔야 하는지 결정하는 문제는 아직 해결이 안됐다고 합니다. 사실 $$H$$라는 고정된 길이로 두는게 상식적으로는 이상하죠. skill 마다 취해야 하는 optimal action의 개수가 다를 테니까요.  
직감적으로는 언제 바꿔야하는지를 결정하는 policy가 하나 더 있어야 할 것 같습니다. 하지만 이 방법을 사용한 모든 연구들은 결과적으로 매 timestep마다 skill을 바꾸게 되는 Collapse에 직면한다고 합니다.  

아래는 PbRL, 중에서도 PEBBLE에 사용되는 reward function 수식입니다. trajectory 2개를 비교해서 binary labeling을 하고, 이를 녹여낸 reward function $$R_{\eta}$$ 를 구해내는 겁니다. 구해낸 reward function과 SAC 알고리즘을 사용해 policy를 학습하면 됩니다.

$$P_{\eta}\left[\tau_{1}^{(z)} \succ \tau_{0}^{(z)}\right]=\frac{\exp \sum_{t} \widehat{R}_{\eta}\left(s_{t}^{1}, z_{t}^{1}\right)}{\sum_{i \in\{0,1\}} \exp \sum_{t} \widehat{R}_{\eta}\left(s_{t}^{i}, z_{t}^{i}\right)}$$

$$\mathcal{L}^{\text {Reward }}=-\mathbb{E}_{\left(\tau^{0}, \tau^{1}, y\right) \sim \mathcal{D}}\left[y(0) \log P_{\eta}\left[\tau_{0}^{(z)} \succ \tau_{1}^{(z)}\right]+y(1) \log P_{\eta}\left[\tau_{1}^{(z)} \succ \tau_{0}^{(z)}\right]\right]$$

<br><br>

## Experiment setup

실험으로는 D4RL suite의 7-DoF robot arm 환경, 그 중에서도 kitchen task가 사용되었습니다. kitchen task는 서랍 여닫기, 불켜기 등을 포함합니다.

![image](https://user-images.githubusercontent.com/57203764/151732135-6fd5398f-e9f9-4a8e-82e8-ce05e0c39254.png?style=centerme){:width="80%"}

<br>

**Offline dataset**  

Skill extraction을 위해서는 expert demo가 담긴 offline dataset이 필요합니다. 본 연구는 Human preference를 통해 noisy dataset에 대해서 robustness를 주장하고 있기 때문에, 고의적으로 expert demo가 아닌 trajectory를 DB에 포함시킵니다. 601개의 잘 학습된 policy를 통해 얻은 trajector, 601개의 random policy trajectory를 합하여 총 1202개 data를 구축하였습니다.  

<br><br>

## Experimental results

실험 결과도 아주 짧고 간단합니다.   

첫 번째로, long-horizon task를 잘 풀 수 있는지에 대한 실험을 진행하였습니다. SkiP의 경우 10%, 즉 120개의 trajectory를 이용하여 preference predictor를 학습한 것이고, Skip 3X의 경우 30%를 사용한 것입니다. 둘 모두 PEBBLE에 비해 높은 성능을 보여주고, 둘 사이의 차이는 거의 나지 않는 것으로 보아 10%의 data도 충분하다고 판단하였습니다.
![image](https://user-images.githubusercontent.com/57203764/151732432-ca45cd90-60a2-4025-9c34-94f84c309fdb.png?style=centerme){:width="80%"}


두 번째로, noisy dataset에 대해서 SkiP이 얼마나 강건하게 skill extraction을 수행하는지 실험하였습니다. 사실 random trajectory를 반이나 섞었기 때문에 일반적인 skill extraction 방법은 당연히 작동하지 않습니다. (적당히 섞으면 비슷할지도 모릅니다....)
![image](https://user-images.githubusercontent.com/57203764/151732577-5202edb3-232c-4be5-85f8-d97633fd8575.png?style=centerme){:width="80%"}

마지막으로, downstream task에 대해서 human preference가 얼마나 효과적인지를 확인했습니다. 사실 이건 PEBBLE에서 이미 성능을 보인 것인데, 차이점이라면 downstream task는 굉장히 쉬운 녀석들이라는 겁니다. 성공/실패의 sparse reward로도 충분히 학습이 가능하니까요. reward shaping 조금만 하면, SkiP보다 더 높은 성능을 보일 거라고 생각합니다.
![image](https://user-images.githubusercontent.com/57203764/151732741-c428314e-d34a-4a07-b091-2e43a5c9f6da.png?style=centerme){:width="80%"}

<br><br>

## Conclusion

Long-horizon task를 PbRL과 skill learning으로 풀어낸 연구였습니다. 사실 PEBBLE 논문을 먼저 읽었다면 그렇게 특별한 점을 찾지 못할 것 같습니다 (안좋은 연구라는게 아니라 반짝반짝 빛나는 연구까지는 아닐지도...라는 말입니다). 그래도 특이한 점은 Human preference를 2 단계에 나눠서 사용하려 했던 점? 정도를 들 수 있겠네요.  