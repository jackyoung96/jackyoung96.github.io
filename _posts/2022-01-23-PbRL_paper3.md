---
layout: post
title: Paper review - Interactive teaching strategies for agent training
tags: archive
---

네 번째 논문 리뷰입니다. 마찬가지로 Preference-based RL이라고는 보기 어렵지만, human interaction RL의 한 종류를 보여주고 있는 논문같아서 리뷰를 시작하게 되었습니다.

2015년 IJCAI에 발표된 논문이라서 DRL을 사용하지는 않았습니다 (DRL이 뜨기 전). 이 논문의 목적은 RL agent가 exploration을 함에 있어서 teacher agent (Human)가 도움을 주고자 하고, 그 과정에서 Laborious human attention을 최소화 하는 것입니다.  

<br>

Table of Contents
   - [Introduction](#introduction)
   - [Student-Teacher RL](#student-teacher-rl)
     * [Teacher-initiated advising](#teacher-initiated-advising)
     * [Student-initiated advising](#student-initiated-advising)
   - [Empirical evaluation](#empirical-evaluation)
   - [Conclusion](#conclusion)
     * [Pros](#pros)
     * [Cons](#cons)

<br><br>

## Introduction

Random policy로부터 학습을 시작하는 것은 굉장히 비효율적입니다. 엄청나게 큰 state space를 exploration하는 과정이 수반되어야 하기 때문이죠. 하지만 사람같은 경우 이미 20년 이상 축적된 meta 지식이 있습니다. 이로부터 도움을 받아 agent가 **좋은 experience buffer를 가질 수 있도록 하는 것**입니다.

> Agents learning how to act in new environments can benefit from input from more experienced agents or humans

이미 환경에 대해서 알고있는 expert (Human이 될 수도 있고, 다른 RL agent 일 수도 있음) 에게서 action을 지도받음으로써 exploration을 원활하게 할 수 있는 방식입니다.  

다만 human expert는 cognitive cost (인지하는데 드는 노동력, 시간 등)를 필요로 합니다. 즉, 이를 최소화 시키는 방법 또한 필요한 것이죠. PbRL의 관점에서는 효율적인 query generation과 같은 맥락입니다.  

즉, 사람이 최대한 적은 시간 관여하는 방법을 찾아내고자 하는 것입니다.
> Aiming to reduce the attention required from the teacher  

<br><br>

## Student-Teacher RL

우선 Teacher는 fixed policy를 가지고 있다고 가정하겠습니다 (특정 preference를 가지고 있다고 할 수 있음). 그런데 이런 의문이 듭니다. Teacher의 policy를 그대로 student의 policy로 쓰면 안될까? Teacher와 Student는 결정적인 차이가 있는데요, 바로 **input state representation**이 다르다는 겁니다.  

예를 들어 Atari game을 할 때, Human은 100 Hz의 시각정보를 바탕으로 게임을 하고, Student는 featurized state를 바탕으로 한다던지 하는 차이입니다. State는 같아도, observation이 다른 것이죠.  

하지만 state는 달라도, action은 동일합니다. Student가 특정 상황에서 취할 action을 Teacher가 선택해 준다면, 더 좋은 데이터가 Buffer에 쌓일테고, 이는 학습 속도를 증가시킬 수 있다는 것이 골자입니다.   

그럼 어떤 방식으로 interaction 하면 좋을까요? 말했듯이 최대한 사람의 품이 덜 들도록 하는 방법을 선택해야 합니다. 아래 예시를 보죠.

> For example, if a person is helping an autonomous car to improve its policy, the overall duration of the teaching period does not matter because the person is always present when the car drives. However, the person might not pay at ention to the road at all times and therefore there is a cost as ociated with monitoring the car’s actions to decide whether to intervene. Moreover, if teaching in this setting requires the human to take control over the car, then providing advice incurs an additional cost beyond monitoring the agent’s behavior (i.e., deciding whether to intervene requires less effort than actually intervening

즉 가끔씩 객관식으로 선택하는게 직접 개입해서 핸들을 조작하는 것보다 사람에게는 쉽다는 것입니다. action을 하나씩 골라주는 것이, action sequence를 제공하거나, Expert demo를 계속 주는 것보다는 직관적으로 쉽다는 거죠.  

<br>
이 Action을 제공하는 방식은 크게 두 가지 방법으로 나뉩니다.

1. Teacher-initiated advising
    
    Teacher 가 계속 관찰하고 있다가 특정 상황이 되면 advise 제공
    
2. Student-initiated advising
    
    Student가 특정 상황에 teacher에게 advise 요청하면 제공

두 방법을 간략히 살펴보겠습니다.  

<br><br>

### Teacher-initiated advising

우선 Importance 라는 value를 아래와 같이 정의합니다.

<center>
$$I(s)=\max _{a} Q_{(s, a)}^{\text {teacher }}-\min _{a} Q_{(s, a)}^{\text {teacher }}$$
</center>

importance가 크다는 것은, 현재의 state에서 택하는 action이 미래의 cumulative reward에 미치는 영향이 크다는 것이므로, 좋은 decision을 내려야 하는 state라는 의미를 내포합니다. 그런데 이 논문에서는 teacher 의 Q value를 어떻게 정의할 수 있는 것인지 나와있지 않습니다. Human Q value function을 정의할 수 없을텐데 말이죠. 아무리 찾아도 알 수 없어서 우선은 넘어갑니다.  

Advise를 제공하는 조건은 두 가지로 나뉩니다. 항상 action을 줘버리면 Imitation learning과 같아버리니까요.

1. Advise Importance
    
    $$I(s)>t_{ti}$$ 일 때 teacher action을 제공함
    
2. Correct importance
    
    $$I(s)>t_{ti}$$ 이고 $$\pi_{teacher}(s)\neq\pi_{student}(s)$$ 일 때 teacher action을 제공함
    
큰 차이는 없어 보이지만, Correct importance가 조건이 하나 더 붙었으니 attention이 조금 덜 할거라고 생각합니다.  
<br>


### Student-initiated advising

이번에는 Student가 먼저 도움을 요청하는 방식입니다. Student는 RL agent이므로 function들이 명확하게 정의되어 있습니다.  

도움을 요청하는 조건은 3가지로 나뉩니다. 

1. Ask Importance
    
    Student importance $$I(s)=\max _{a} Q_{(s, a)}^{\text {student}}-\min _{a} Q_{(s, a)}^{\text {student}} > t_{ti}$$ 일 때 요청  
    → Student가 생각할 때 중요한 결정을 내려야 하는 state에 요청
    
2. Ask Uncertainty
    
    $$\max _{a} Q_{(s, a)}^{\text {student }}-\min _{a} Q_{(s, a)}^{\text {student }}<t_{\text {unc }}$$ 일 때 요청  
    → Student 가 헷갈릴 때 요청
    
3. Ask Unfamiliar
    
    $$\operatorname{distance}(s, N N(s))>t_{u n f}$$ 일 때 요청  
    → 이전에 가본 적이 없을 때 요청
    
아주 심플한 구성입니다. 다만 PbRL의 관점에서 보았을 때 Query를 어떻게 선택하면 좋을까 라는 질문에 좋은 대답이 될 것 같습니다. RL agent가 특정 조건에만 Query를 생성한다면 확실히 Human labor가 줄어들테니까요.  

<br><br>

## Empirical evaluation

사실 제가 알고 싶었던 것은, 수식과 정확한 실험 방법이었는데, 논문에는 자세히 소개되지 않았습니다. 단순한 empirical evaluation만 진행되어서 아쉬움이 있습니다.  

Packman 환경에서 실험을 수행하였고, RL 알고리즘으로는 SALSA를 사용하였습니다. DRL이 나오기 전 논문이니 어쩔 수 없습니다. 그때 당시로는 최신 알고리즘입니다.  

결론은 Ask Importance - Correct Importance 방식이, 가장 효과적으로 reward를 증가시키면서 human attention을 감소시켰습니다. 아래 그래프에서 확인이 가능합니다. (그래프도 약간 무성의하게 그려놔서 화가 나네요^^)

![image](https://user-images.githubusercontent.com/57203764/150701733-b4d6e35c-f896-44c3-b20c-789474fe39a4.png?style=centerme){:width="70%"}


그래프가 보여주는 것은 명확합니다. 단순 Correct Importance 방식은 Teacher가 계속 보고 있어야 해서 cumulative attention이 빠르게 증가하는데, Student-initialized 방식을 사용하면 비슷한 reward 수준에 도달하면서 attention을 확실히 줄일 수 있다는 것입니다.  

한 가지 주목할 점은, Fig 3, 4를 비교해보면 pre-training의 episode 개수만 다르고 동일한 실험을 진행한 것입니다. 이 때 pre-training episode를 늘리자 attention의 scale이 폭발적으로 증가한 것을 볼 수 있습니다. 다시 말해 pre-training을 시켜놓으면 사람이 원하는 대로 되돌리기가 어려울 수도 있다는 거죠. 백지인 녀석이 가르치기 편한 것과 같은 논리입니다. (강백호가 떠오르네요)  

<br><br>

## Conclusion

논문이 생각보다 너무 간단해서, 제가 생각하는 장단점을 좀 적어보고 마무리 하겠습니다. 조금 아쉬운 느낌이 있네요. 다음부터는 훨씬 유명한 논문을 리뷰해야 할 것 같습니다.

### Pros

1. Expert interaction이라는 관점에서 preference를 녹여낼 수 있겠다는 생각도 들었음. Action을 직접 지정해주는 preference (다만 엄청나게 expensive 할 것)
2. Preference query를 잘 선택하는 방법에 대해서 insight를 얻을 수 있음
3. Pre-training이 preference 학습에는 악영향을 줄 수도 있겠다는 생각이 듦

### Cons

1. Teacher Q value를 어떻게 정의하는지 밝히지 않음
2. Teacher 가 정확히 어떻게 개입하는지 밝히지 않음
3. Teacher의 개입으로 인해서 corner case failure가 발생할텐데, 이를 고려하지 않음


