---
layout: post
title: Paper review - Learning trajectory preferences for manipulators via iterative improvement
---

두 번째 논문 리뷰입니다. 2013년 NeurIPS에 제출된 논문인 Learning trajectory preferences for manipulators via iterative improvement 를 리뷰해 보도록 하겠습니다.  
PbRL 자체가 오래된 연구 분야는 아닌 것 같아서, 2013년 논문이면 정말 고대 논문이라고 볼 수 있겠습니다. 다만, trajectory planning을 사용했다는 내용이 리뷰논문에 있었기 때문에, 첫 번째로 채택되었습니다.  

실제로 Manipulator에서 human demonstration을 사용하는 연구는 유명했었죠. (로봇 이름이 뭐였는지는 기억이 잘 안나지만...)

[![image](https://user-images.githubusercontent.com/57203764/148876845-b50a671f-558f-42cb-8691-8951700e2d8a.png)](https://www.youtube.com/watch?v=M413lLWvrbI)

본 논문의 경우 **High DoF 환경**에서는 Human demo보다는 slightly improved trajectory preference를 주는 것이 학습에 도움이 된다고 주장합니다. 그리고 그 방법과 결과를 살펴보겠습니다.  
<br/><br/>

## Introduction
---
본 논문은 human preference가 필요한 이유를 여러 예시를 들어 잘 정의하고 있습니다.
> a household robot should move a glass of water in an upright position without jerks while maintaining a safe distance from nearby electronic devices  

> For example, trajectories of heavy items should not pass over fragile items but rather move around them  

같은 물건을 옮기더라도 어떤 물건이느냐, 주변에 어떤 것이 있느냐에 따라서 다르게 행동해야 한다는 것이 이 연구의 요지입니다. 이것은 reward function으로 나타내기 어려운 사람의 선호도 (Human preference) 이고, 이를 로봇이 학습할 수 있도록 하는 것입니다.  
> The robot learns a general model of the user's preferences in an online fashion.

이 과정에서 학습이 잘 되는지를 사실 검증할 수 없는데, 저자는 regret이라는 것을 정의하였습니다. 사람과 로봇이 생각하는 trajectory의 rank이 같아질수록 regret이 작아지는 구조입니다 (뒷부분에 설명)  

Grocery checkout task라는, 여러 물체를 두고 로봇팔을 이용해 이것들을 옮기는 문제를 실험환경으로 사용했습니다.  

<br/><br/>

## Related works
---
본 논문은 Learning from demonstratino (LfD)와 많은 비교를 하고 있습니다. LfD의 가장 큰 문제는 과연 user demonstration이 **optimal**인지 알 수 없다는 것입니다. 
> The user never discloses the optimal trajectory

**즉, 본 연구의 목적은 어떻게 improve 하는지를 preference에 기반하여 배우는 것 입니다.**  
> Learning a score function representing the preferences in trajectories

이 때 PbRL의 방법 중 하나였던 utility function 과 유사하게 score function을 구하는 것을 목표로 하는 것입니다.  

<br/><br/>

## Learning and Feedback model
---
우선 우리가 배우고자 하는 scoring function을 $$s(x,y;w)$$라고 하겠습니다. $$x$$는 context, $$y$$는 trajectory, 그리고 $$w$$는 우리가 학습하고자 하는 parameter입니다. Human preference 가 반영된 optimal scoring function은 $$s^*(x,y)$$로 나타내겠습니다.

Scoring function을 학습하는 과정은 3 step으로 나뉩니다.

1. Step 1: The robot receives a context x. It then uses a planner to sample a set of trajectories, and ranks them according to its current approximate scoring function s(x, y; w).   
   로봇이 x라는 context를 기반으로 multiple trajectory를 형성합니다. 그리고 scoring function으로 이들에게 rank를 부여합니다.
2. Step 2: The user either lets the robot execute the top-ranked trajectory, or corrects the robot by providing an improved trajectory y¯. This provides feedback indicating that s∗(x, y¯) > s∗(x, y).   
    가장 상위의 trajectory를 선택해 rank를 수정해 줌으로써 피드백을 부여합니다.
3. Step 3: The robot now updates the parameter w of s(x, y; w) based on this preference feedback and returns to step  
    scoring function을 update합니다.

참으로 쉬운 구조임이 분명합니다. 사실 알파고 논문이 나오기도 전의 논문이기 때문에, 현재의 딥러닝 관점에서는 허접해 보일수도 있습니다만, 이러한 이론이 배경이 되지 않았다면 딥러닝 또한 발전할 수 없었을 겁니다.  

마지막으로 Performance evaluation을 위해 Regret을 정의합니다.
<center>
$$REG_T=\frac{1}{T}\sum_{t=1}^T[s^*(x_t,y_t^*)-s^*(x_t,y_t)]$$
$$where\ y^*_t=argmax_ys^*(x_t,y)$$
</center>

그런데 뭔가 이상합니다. 사실 $$s^*$$는 알 수 없는 값이기 때문에 Regret을 구할 수 없습니다. 그래서 저자는 regret bound를 이용하여 수렴성을 증명합니다. (이후에 나옴)  

Human feedback을 받기 위해서는 UI/UX가 필요합니다. 저자는 OpenRave라는 프로그램을 사용했다고 합니다. Multiple trajectories 중 하나를 클릭하면 그것의 rank가 가장 높아지도록 설정되었습니다.  

<br/><br/>

## Learning algorithm
---
