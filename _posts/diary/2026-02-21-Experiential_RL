---

---

# Experiential Reinforcement Learning (ERL) ë¦¬ë·°: Rolloutì˜ ìˆ¨ê²¨ì§„ ì •ë³´ë¥¼ í™œìš©í•˜ì

> **TL;DR**: On-policy RLì—ì„œ rollout ê³¼ì •ì˜ ì •ë³´ ë‚­ë¹„ ë¬¸ì œë¥¼ ì§€ì í•˜ê³ , self-reflectionì„ í†µí•´ exploration íš¨ìœ¨ì„ ê·¹ì ìœ¼ë¡œ ë†’ì´ëŠ” Experiential Reinforcement Learning (ERL) íŒ¨ëŸ¬ë‹¤ì„ ì œì•ˆ

## ë“¤ì–´ê°€ë©°: ìš°ë¦¬ëŠ” ì´ë¯¸ ì´ê±¸ í•˜ê³  ìˆë‹¤

ì´ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œ ê°€ì¥ ë¨¼ì € ë“  ìƒê°ì€ "ì•„, ìš°ë¦¬ê°€ ì‹¤ë¬´ì—ì„œ ì´ë¯¸ í•˜ê³  ìˆëŠ” ê²ƒì„ í˜•ì‹í™”í–ˆêµ¬ë‚˜"ì˜€ë‹¤.

**í˜„ì‹¤ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì„ ìƒê°í•´ë³´ì:**

1. ìƒˆë¡œìš´ ëª¨ë¸ì„ ë°°í¬í•œë‹¤
2. ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ë©´ì„œ ë¬¸ì œì ì„ ë°œê²¬í•œë‹¤
3. Promptingì´ë‚˜ agent systemìœ¼ë¡œ self-reflectionì„ í†µí•´ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦°ë‹¤
4. ì´ë ‡ê²Œ ëª¨ì¸ ë°ì´í„°ë¡œ SFTë¥¼ í•˜ê±°ë‚˜ RLì„ ëŒë¦°ë‹¤
5. ê°œì„ ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë°°í¬í•œë‹¤
6. **1-5ë¥¼ ë°˜ë³µí•œë‹¤**

ERLì€ ë°”ë¡œ ì´ cycleì„ training loop ì•ˆì— ë‚´ì¬í™”ì‹œí‚¨ ê²ƒì´ë‹¤. ì‹¤ë¬´ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ í•˜ë˜ "ê²½í—˜ â†’ ë°˜ì„± â†’ ê°œì„  â†’ ë‚´ì¬í™”"ì˜ íë¦„ì„ RL í•™ìŠµ ê³¼ì •ì— ë…¹ì—¬ë‚¸ ì…ˆì´ë‹¤.

## ë¬¸ì œ ì œê¸°: Rolloutì€ ë¹„ì‹¸ê³ , RewardëŠ” ë„ˆë¬´ ë‹¨ìˆœí•˜ë‹¤

On-policy RLì„ NLPì— ì ìš©í•  ë•Œ ê°€ì¥ í° ë³‘ëª©ì´ ë­˜ê¹Œ?

**Rolloutì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦°ë‹¤.** ëª¨ë¸ì´ ì „ì²´ responseë¥¼ ìƒì„±í•˜ê³ , í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ê³ , trajectoryë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì€ ê³„ì‚°ì ìœ¼ë¡œ ë§¤ìš° ë¹„ì‹¸ë‹¤.

**ê·¸ëŸ°ë° ê·¸ ê²°ê³¼ë¬¼ì—ì„œ ì–»ëŠ” í”¼ë“œë°±ì€?** ë‹¨ìˆœí•œ scalar rewardë¿ì´ë‹¤.

ì´ê²Œ ì–¼ë§ˆë‚˜ ë‚­ë¹„ì ì¸ì§€ ìƒê°í•´ë³´ì. Reasoning modelì˜ ê²½ìš° ìƒì„±ëœ responseì—ëŠ” **ì—„ì²­ë‚˜ê²Œ í’ë¶€í•œ ì •ë³´**ê°€ ë‹´ê²¨ ìˆë‹¤:
- ì–´ë””ì„œ ì‹¤ìˆ˜í–ˆëŠ”ì§€
- ì–´ë–¤ reasoning pathë¥¼ ë”°ëëŠ”ì§€
- ì–´ë–¤ ì „ëµì´ ì‘ë™í•˜ì§€ ì•Šì•˜ëŠ”ì§€

í•˜ì§€ë§Œ ê¸°ì¡´ RLVR(Reinforcement Learning with Verifiable Rewards)ëŠ” ì´ ëª¨ë“  ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê³  "ë§ì•˜ë‹¤/í‹€ë ¸ë‹¤"ë¼ëŠ” ë‹¨ìˆœí•œ ì‹ í˜¸ë§Œ ì‚¬ìš©í•œë‹¤. ëª¨ë¸ì€ implicití•˜ê²Œ "ëŒ€ì²´ ë­˜ ì˜ëª»í–ˆì§€?"ë¥¼ ì¶”ë¡ í•´ì•¼ í•œë‹¤.

## ERLì˜ í•µì‹¬: Experience-Reflection-Consolidation Loop

ERLì€ ì´ ë¬¸ì œë¥¼ ì •ë©´ìœ¼ë¡œ í•´ê²°í•œë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ”:

### 1ë‹¨ê³„: First Attempt
```
x (task) â†’ Ï€Î¸ â†’ y(1) (first attempt) â†’ (f(1), r(1)) (feedback, reward)
```

ì¼ë‹¨ ì²« ë²ˆì§¸ ì‹œë„ë¥¼ í•œë‹¤. ì—¬ê¸°ê¹Œì§„ ê¸°ì¡´ RLê³¼ ë™ì¼í•˜ë‹¤.

### 2ë‹¨ê³„: Self-Reflection
```
(x, y(1), f(1), r(1), m) â†’ Ï€Î¸ â†’ Î” (reflection)
```

ì—¬ê¸°ê°€ í•µì‹¬ì´ë‹¤. ì²« ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´, ëª¨ë¸ì´ **ìŠ¤ìŠ¤ë¡œ** "ë­˜ ì˜ëª»í–ˆê³  ì–´ë–»ê²Œ ê³ ì³ì•¼ í•˜ëŠ”ì§€" reflectionì„ ìƒì„±í•œë‹¤. `m`ì€ cross-episode memoryë¡œ, ì´ì „ì— íš¨ê³¼ì ì´ì—ˆë˜ corrective patternì„ ì €ì¥í•œë‹¤.

### 3ë‹¨ê³„: Second Attempt (Retry)
```
(x, Î”) â†’ Ï€Î¸ â†’ y(2) â†’ (f(2), r(2))
```

Reflectionì„ conditioningìœ¼ë¡œ ë‘ ë²ˆì§¸ ì‹œë„ë¥¼ í•œë‹¤. Reflectionì´ íš¨ê³¼ì ì´ë¼ë©´ ì„±ëŠ¥ì´ ì˜¬ë¼ê°„ë‹¤.

### 4ë‹¨ê³„: Internalization (í•µì‹¬!)
```
â„’_distill(Î¸) = -ğ”¼[ğ•€(r(2) > 0) log Ï€Î¸(y(2)|x)]
```

**ì´ê²Œ ì§„ì§œ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë‹¤.** ì„±ê³µí•œ ë‘ ë²ˆì§¸ ì‹œë„ë¥¼ **reflection ì—†ì´** ì›ë˜ inputë§Œìœ¼ë¡œ ì¬í˜„í•˜ë„ë¡ distillationí•œë‹¤.

ì™œ ì´ê²Œ ì¤‘ìš”í•˜ëƒë©´:
- **Inference timeì— reflectionì´ í•„ìš” ì—†ë‹¤**: í•™ìŠµ ì¤‘ì— ì–»ì€ insightê°€ base policyì— ë‚´ì¬í™”ëœë‹¤
- **ë°°í¬ í›„ ì¶”ê°€ ë¹„ìš©ì´ ì—†ë‹¤**: Self-reflectionì„ ëŒë¦´ í•„ìš” ì—†ì´ ë°”ë¡œ ê°œì„ ëœ í–‰ë™ì„ í•œë‹¤

## ì‹¤í—˜ ê²°ê³¼: ì–´ë””ì„œ íš¨ê³¼ê°€ í´ê¹Œ?

| Environment | Qwen3-4B (RLVR â†’ ERL) | Olmo3-7B (RLVR â†’ ERL) |
|------------|----------------------|----------------------|
| Sokoban | 0.06 â†’ 0.87 (+81%) | 0.04 â†’ 0.20 |
| FrozenLake | 0.72 â†’ 0.91 (+27%) | 0.70 â†’ 0.90 |
| HotpotQA | 0.55 â†’ 0.61 (+11%) | 0.51 â†’ 0.57 |

**ê°€ì¥ íš¨ê³¼ê°€ í° ê³³**: Sokoban, FrozenLake ê°™ì€ sparse-reward + unknown dynamics í™˜ê²½

ì´ìœ ê°€ ëª…í™•í•˜ë‹¤:
1. **í™˜ê²½ ê·œì¹™ì„ ì§ì ‘ ì•Œë ¤ì£¼ì§€ ì•ŠëŠ”ë‹¤**: ëª¨ë¸ì´ trial-and-errorë¡œ ì¶”ë¡ í•´ì•¼ í•¨
2. **Long-horizon planning í•„ìš”**: ì‘ì€ ì‹¤ìˆ˜ê°€ ëˆ„ì ë˜ì–´ ì‹¤íŒ¨ë¡œ ì´ì–´ì§
3. **Sparse reward**: ìµœì¢… ê²°ê³¼ì—ì„œë§Œ í”¼ë“œë°±ì„ ë°›ìŒ

ì´ëŸ° ì¡°ê±´ì—ì„œëŠ” "implicití•˜ê²Œ ë­˜ ì˜ëª»í–ˆëŠ”ì§€ ì¶”ë¡ "í•˜ëŠ” ê²Œ ê±°ì˜ ë¶ˆê°€ëŠ¥í•˜ë‹¤. í•˜ì§€ë§Œ explicit reflectionì´ ìˆìœ¼ë©´ "ì•„, ì €ê¸°ì„œ ì™¼ìª½ìœ¼ë¡œ ê°”ì–´ì•¼ í–ˆëŠ”ë°"ë¼ëŠ” êµ¬ì²´ì ì¸ êµì •ì´ ê°€ëŠ¥í•˜ë‹¤.

ë°˜ë©´ HotpotQAëŠ” ê°œì„ í­ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ë‹¤. Tool-usingì´ ì£¼ëœ taskë¼ interaction patternì´ homogeneousí•˜ê³ , rewardë„ ë” denseí•˜ê¸° ë•Œë¬¸ì´ë‹¤.

## ì™œ ì´ê²Œ ì˜ë¯¸ìˆë‚˜: ì‹¤ë¬´ì  ê´€ì 

### 1. Agent Systemì˜ ë¯¸ë˜ë¥¼ ë³´ì—¬ì¤€ë‹¤

í˜„ì¬ ìš°ë¦¬ê°€ agentë¥¼ ë§Œë“¤ ë•Œ:
```
Base Model â†’ Promptingìœ¼ë¡œ ì„±ëŠ¥ ëŒì–´ì˜¬ë¦¼ â†’ ë°ì´í„° ìˆ˜ì§‘ â†’ Fine-tuning â†’ ë°˜ë³µ
```

ERLì€ ì´ cycleì„ **ìë™í™”**í•œë‹¤. Agentê°€ ìŠ¤ìŠ¤ë¡œ:
- ê²½í—˜ì—ì„œ ë°°ìš°ê³ 
- ë°˜ì„±í•˜ê³ 
- ë‚´ì¬í™”í•œë‹¤

### 2. Inference Cost ì ˆê°

Self-reflectionì„ inference timeì— í•˜ë©´ ë¹„ìš©ì´ 2ë°°ë‹¤. í•˜ì§€ë§Œ ERLì˜ internalization step ë•ë¶„ì— ë°°í¬ëœ ëª¨ë¸ì€ reflection ì—†ì´ ë°”ë¡œ ê°œì„ ëœ í–‰ë™ì„ í•œë‹¤.

### 3. Exploration Efficiency

ê¸°ì¡´ RLì€ "ì¢‹ì€ trajectoryë¥¼ ìš°ì—°íˆ ë°œê²¬"í•´ì•¼ í•œë‹¤. ERLì€ ì‹¤íŒ¨í•œ trajectoryì—ì„œë„ **êµ¬ì¡°í™”ëœ ê°œì„  ë°©í–¥**ì„ ì¶”ì¶œí•œë‹¤. ì´ê²Œ sample efficiencyë¥¼ ê·¹ì ìœ¼ë¡œ ë†’ì¸ë‹¤.

## í•œê³„ì™€ ìƒê°í•  ì 

### Memoryê°€ ì–‘ë‚ ì˜ ê²€
Cross-episode memoryê°€ ì˜ëª»ëœ reflectionì„ ì €ì¥í•˜ë©´ ì˜¤íˆë ¤ ë°©í•´ê°€ ëœë‹¤. Olmo3-7B Sokobanì—ì„œ no-memory variantê°€ full ERLì„ ì•½ê°„ ì´ê²¼ë‹¤. ëª¨ë¸ì˜ self-reflection ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë©´ memoryê°€ noiseë¥¼ ì¶•ì í•  ìˆ˜ ìˆë‹¤.

### Self-Reflection ëŠ¥ë ¥ ì˜ì¡´ì„±
ê²°êµ­ ëª¨ë¸ì´ "ì¢‹ì€ reflection"ì„ ìƒì„±í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ì´ê±´ base modelì˜ ëŠ¥ë ¥ì— í¬ê²Œ ì˜ì¡´í•œë‹¤.

### Compute Budget
ë‘ ë²ˆ attempt + reflection ìƒì„± = rollout ë¹„ìš© ì¦ê°€. ë…¼ë¬¸ì—ì„œëŠ” RLVRì— ë” ë§ì€ rolloutì„ í• ë‹¹í•´ì„œ fair comparisonì„ í–ˆì§€ë§Œ, ì‹¤ë¬´ì—ì„œëŠ” trade-off ê³ ë ¤ í•„ìš”.

## ë§ˆë¬´ë¦¬

ì´ ë…¼ë¬¸ì´ ì§€ì í•˜ëŠ” í•µì‹¬ ë¬¸ì œëŠ” ëª…í™•í•˜ë‹¤:

> **On-policy RLì—ì„œ rolloutì€ ê°€ì¥ ë¹„ì‹¼ ì—°ì‚°ì¸ë°, ê±°ê¸°ì„œ ë‚˜ì˜¤ëŠ” í’ë¶€í•œ ì •ë³´ë¥¼ scalar rewardë¡œ ì••ì¶•í•´ë²„ë¦¬ëŠ” ê±´ ë‚­ë¹„ë‹¤.**

ê·¸ë¦¬ê³  í•´ê²°ì±…ë„ ì§ê´€ì ì´ë‹¤:

> **Generated responseì— ë‹´ê¸´ ì •ë³´ë¥¼ explicit reflectionìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ì„±ê³µí•œ êµì •ì„ base policyì— ë‚´ì¬í™”í•˜ì.**

ì‚¬ì‹¤ ì´ê±´ ì¸ê°„ì´ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ë˜‘ê°™ë‹¤. ì‹¤ìˆ˜í•˜ê³ , ì™œ ì‹¤ìˆ˜í–ˆëŠ”ì§€ ë°˜ì„±í•˜ê³ , ë‹¤ìŒì— ë” ì˜í•˜ë ¤ê³  ë…¸ë ¥í•˜ê³ , ê²°êµ­ ì˜ì‹í•˜ì§€ ì•Šì•„ë„ ì˜¬ë°”ë¥¸ í–‰ë™ì´ ìì—°ìŠ¤ëŸ¬ì›Œì§„ë‹¤.

ERLì€ ì´ "ê²½í—˜ì  í•™ìŠµ"ì„ LLM trainingì— formalizeí•œ ê²ƒì´ë‹¤. Agent systemì´ ì ì  ë³µì¡í•´ì§€ëŠ” ì‹œëŒ€ì—, ì´ëŸ° ë°©í–¥ì˜ ì—°êµ¬ê°€ ë” ë§ì´ ë‚˜ì˜¬ ê²ƒ ê°™ë‹¤.

---

## References
- Paper: [Experiential Reinforcement Learning (arXiv:2602.13949)](https://arxiv.org/abs/2602.13949)
- Authors: Taiwei Shi, Sihao Chen, Bowen Jiang, Linxin Song, Longqi Yang, Jieyu Zhao
- Affiliations: USC, Microsoft, UPenn
