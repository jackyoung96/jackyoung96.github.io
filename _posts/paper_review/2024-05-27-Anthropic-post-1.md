---
layout: post
title: Anthropic post - Mapping the Mind of a Large Language Model
tags: archive
---

ì´ë¦„ë¶€í„°ê°€ ì•„ì£¼ í¥ë¯¸ë¡œìš´ Anthropic ì˜ í¬ìŠ¤íŒ…

## í¬ìŠ¤íŒ…ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½

> ğŸ¬ **ML ëª¨ë¸ì˜ ë‚´ë¶€ë¥¼ ì‚´í´ë³´ëŠ” ì¼ì€ í¬ê²Œ ì˜ë¯¸ëŠ” ì—†ëŠ” ì¼ì´ì—ˆë‹¤. ì˜ë¯¸ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ìˆ«ìì˜ ë‚˜ì—´ì´ê¸° ë•Œë¬¸ì´ë‹¤.**  
> Opening the black box doesn't necessarily help: the internal state of the modelâ€”what the model is "thinking" before writing its responseâ€”consists of a long list of numbers ("neuron activations") without a clear meaning

> ğŸ¬ **â€œDictionary learningâ€
Many unclear active neuron ëŒ€ì‹  few active features ì˜ dictionary ë¥¼ ë§Œë“¤ì–´ ì˜ë¯¸ë¥¼ ê´€ì¸¡í•œë‹¤**  
> In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.

> ğŸ¬ Base of dictionary learning  
> **Sparse autoEncoder ë¥¼ ì‚¬ìš©í•˜ì—¬ neuron activation ìœ¼ë¡œë¶€í„° feature ë¥¼ ì¶”ì¶œí•œë‹¤**  
> Sparse AutoEncoder ë€? [https://soki.tistory.com/64](https://soki.tistory.com/64)  
> Anthropic ì˜ feature decomposing: [https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)  
> Superposition hypothesis: Model ì´ Neuron ì˜ ìˆ˜ë³´ë‹¤ ë” ë§ì€ íŠ¹ì§•ì„ í‘œí˜„í•˜ëŠ” ìƒíƒœì„ì„ ê°€ì • â†’ ê³ ì°¨ì› ê³µê°„ì˜ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë” í° ì‹ ê²½ë§ì„ estimate í•œë‹¤ëŠ” ê°€ì •  
> **feature ê³µê°„ì˜ dimension ì€ neuron activation demension ë³´ë‹¤ ì»¤ì•¼ í•¨**  
> ë”°ë¼ì„œ Sparse AutoEncoder ë¥¼ ì‚¬ìš©í•œë‹¤ â†’ Latent dimension ê°€ input dimension ë³´ë‹¤ 256ë°° í¬ë‹¤


> ğŸ¬ **Feature distance  
> Neuron activation patterns ë¥¼ ê¸°ë°˜ìœ¼ë¡œ feature ì˜ distance ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.**  
> We were able to measure a kind of "distance" between features based on which neurons appeared in their activation patterns.  
> **Golden Gate Bridge ë¼ëŠ” ëª…ì‚¬ëŠ” Alcatraz Island, Ghirardelli Square, the Golden State Warriors ë“±ì˜ ëª…ì‚¬ì™€ ìœ ì‚¬í•œ feature ë¥¼ ê°€ì§„ë‹¤**  
> This allowed us to look for features that are "close" to each other. Looking near a "Golden Gate Bridge" feature, we found features for Alcatraz Island, Ghirardelli Square, the Golden State Warriors, California Governor Gavin Newsom, the 1906 earthquake, and the San Francisco-set Alfred Hitchcock filmÂ *Vertigo*.  
> **Inner conflict ëŠ” catch-22 (ì˜ˆìƒí–ˆë˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ëŠ” ê¶ì§€ìƒíƒœ' ë‚˜ 'ë”œë ˆë§ˆ' ë˜ëŠ” 'êµ‰ì¥íˆ ì–´ë ¤ìš´ ìƒí™©â€™) ê³¼ ê°™ì€ ë¹„ìœ ì  í‘œí˜„ê³¼ë„ ìœ ì‚¬í•œ feature ë¥¼ ê°€ì§„ë‹¤â†’ Claude ì˜ ë¹„ìœ ì™€ ì€ìœ  ëŠ¥ë ¥ì„ ì–´ëŠì •ë„ ì„¤ëª…)**  
> looking near a feature related to the concept of "inner conflict", we find features related to relationship breakups, conflicting allegiances, logical inconsistencies, as well as the phrase "catch-22â€

> ğŸ¬ **Manipulate features  
> íŠ¹ì • feature ë¥¼ activate í•˜ë©´ model responseë¥¼ control í•  ìˆ˜ ìˆìŒ**  
> But when we ask the same question with the feature artificially activated sufficiently strongly, this overcomes Claude's harmlessness training and it responds by drafting a scam email.  
> **Input context ì— ë°˜ì‘í•˜ëŠ” ê²ƒì´ ì´ëŸ¬í•œ feature activation ê³¼ ê´€ë ¨ ìˆì„ ë¿ ì•„ë‹ˆë¼, ëª¨ë¸ì´ feature ë¥¼ í†µí•´ world representation ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ behavior ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„**  
> The fact that manipulating these features causes corresponding changes to behavior validates that they aren't just correlated with the presence of concepts in input text, but also causally shape the model's behavior.


> ğŸ¬ **Make the model safer**  
> **Feature ê°ì§€ë¥¼ í†µí•´ AI monitoring ì´ ê°€ëŠ¥í•˜ê³ , íŠ¹ì • ì£¼ì œë¥¼ ì œê±°í•˜ëŠ” ë“±ì˜ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥í•¨**  
> For example, it might be possible to use the techniques described here to monitor AI systems for certain dangerous behaviors (such as deceiving the user), to steer them towards desirable outcomes (debiasing), or to remove certain dangerous subject matter entirely.

### ìš”ì•½

- MLP layerì˜ activation patternì„ Sparse AutoEncoder ë¥¼ í†µí•´ feature í™” í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ê³ ì „ ML ì—ì„œ ì‚¬ìš©ë˜ë˜ Dictionary learning ë°©ì‹ì´ë‹¤
- LLM ì— Dictionary learning ë°©ì‹ì„ ì‚¬ìš©í–ˆë”ë‹ˆ ì—°ê´€ëœ ë‹¨ì–´ ë¿ë§Œ ì•„ë‹ˆë¼ ìœ ì‚¬í•œ high-level ê°œë…ì´ë‚˜ ì€ìœ /ë¹„ìœ  ë“±ì˜ ëª¨í˜¸í•œ ê°œë…ë“¤ë„ feature ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìˆìŒì´ ë°œê²¬ë˜ì—ˆë‹¤.
- íŠ¹ì • feature ë¥¼ ì¼ë¶€ëŸ¬ ê°•í™”í•˜ë©´ í•´ë‹¹ feature ì™€ ìœ ì‚¬í•œ ë‹µë³€ì´ ìƒì„±ë˜ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•˜ì˜€ë‹¤. ì´ëŠ” in-context learning ë“±ì´ íŠ¹ì • feature ë¥¼ ê°•í™”ì‹œì¼œì„œ í•´ë‹¹ ë‚´ìš©ê³¼ ì—°ê´€ëœ ë‹µë³€ì„ í•˜ëŠ” ê²ƒì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. **LLM ì€ feature ë¥¼ í†µí•´ì„œ world representation ì„ ì´í•´í•˜ê³  ìˆë‹¤!**
- íŠ¹ì • Input/Output ì´ ì–´ë–¤ feature ì™€ ê°€ê¹Œìš´ì§€ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— **LLM ì„ ë” safe í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.**

### Anthropic ì˜ dictionary features

[https://transformer-circuits.pub/2023/monosemantic-features/vis/index.html](https://transformer-circuits.pub/2023/monosemantic-features/vis/index.html)

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/49431c8f-959f-4bdc-a1f1-a21ad96e519c)
