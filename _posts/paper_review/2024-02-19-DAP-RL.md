---
layout: post
title: Direct Alignment from Preferences Part 01. RLHF
tags: archive
---

# Introduction

2023 ë…„ì€ ê°íˆ chatGPT ì˜ ì‹œëŒ€ì˜€ë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. chatGPT ì˜ ì„±ê³µì—ëŠ” ë‹¤ì–‘í•œ ìš”ì†Œë“¤ì´ ìˆìŠµë‹ˆë‹¤. GPU ì˜ ë°œì „ê³¼ í•¨ê»˜ ì—„ì²­ë‚˜ê²Œ ê±°ëŒ€í•œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ë©´ì„œ 175B, ê·¸ ì´ìƒì˜ ëª¨ë¸ì´ ë“±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜ ë‹¨ìˆœíˆ ë§ì€ ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” ê²ƒì„ ë„˜ì–´ ë†’ì€ í€„ë¦¬í‹°ì˜ ì •ì œëœ Instruction ë°ì´í„°ì…‹ë“¤ì„ ì‚¬ìš©í•˜ë©´ì„œ ì‚¬ëŒê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì±„íŒ… ëª¨ë¸ë¡œì¨ì˜ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë ¸ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì€ ìš”ì¸ë“¤ ì¤‘ ë‹¨ í•˜ë‚˜ì˜ ìš”ì¸ì„ ê¼½ì•„ë³´ë¼ë©´ ì €ëŠ” RLHF, ì¦‰ **Model alignment** ë¥¼ ê¼½ê² ìŠµë‹ˆë‹¤.  

Model alignment ëŠ” ê°„ë‹¨íˆ ë§í•´ "**ì‚¬ëŒì˜ ì·¨í–¥ì— ë§ëŠ” ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµ**"í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ SFT (Supervised Fine-Tuning) ëŠ” ëª¨ë¸ì´ í•™ìŠµë°ì´í„°ì™€ ê°™ì€ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ SFTë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë¯¸ì²˜ í•™ìŠµí•˜ì§€ ëª»í•œ ë°ì´í„°ì— ëŒ€í•´ ì ì ˆí•˜ê²Œ ëŒ€ì‘í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë˜í•œ, í•™ìŠµ ë°ì´í„°ì— ì ì ˆì¹˜ ëª»í•œ ë‹µë³€ì´ í¬í•¨ëœë‹¤ë©´ ë¶€ì ì ˆí•˜ê±°ë‚˜ ì™œê³¡ëœ ë‹µë³€ì„ ìƒì„±í•  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Model alignment ëŠ” **ì¸ê°„ì˜ í”¼ë“œë°±**ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì˜ ë‹µë³€ì„ ê°œì„ í•©ë‹ˆë‹¤.

chatGPT ì—ì„œ ì‚¬ìš©ëœ Model alignment ë°©ì‹ì€ RLHF ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ RM (Reward Model) ì˜ í•™ìŠµê³¼ RL (Reinforcement Learning) ì„ ì´ìš©í•œ í•™ìŠµë°©ë²•ìœ¼ë¡œ ë„ë¦¬ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ RL ì€ ë§¤ìš° ë¶ˆì•ˆì •í•œ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ RL ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ëŠ” ì—¬ëŸ¬ ë°©ë²•ë¡ ë“¤ì´ ë“±ì¥í–ˆê³ , ì´ë“¤ì„ DAP (Direct Alignment from Preference) ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

ì´ë²ˆ í¬ìŠ¤íŒ… 3ë¶€ì‘(ì•„ë§ˆ?)ì—ì„œëŠ” RLê³¼ DAP ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì•Œì•„ë³´ê³ , ìµœì‹  RL, ê·¸ë¦¬ê³  DAP ë°©ë²•ë¡ ë“¤ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Part 1 ì€ RL ë°©ë²•ë¡ ë“¤ì…ë‹ˆë‹¤.

# Model alignment - RL

## RLHF 

OpenAI ê°€ 2022ë…„ì— ë‚´ë†“ì€ [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) (Ouyang et al., 2022) ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì‚¬ì‹¤ ê·¸ ì „ì—ë„ [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) (Stiennon et al., 2020) ë…¼ë¬¸ì„ í†µí•´ Summarization task ì— ëŒ€í•´ ë™ì¼í•œ ë°©ë²•ì„ ì œì•ˆí–ˆì§€ë§Œ, chatGPT ë§Œí¼ì˜ íŒŒê¸‰ë ¥ì„ ë³´ì´ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/12175347-5cc6-4cea-9a7f-07e8118130db)

RLHF ëŠ” 3ê°œì˜ step ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.
1) SFT: Chat í˜•íƒœë¡œ êµ¬ì¶•ëœ ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
2) RM: í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ê°œì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  ì‚¬ëŒì€ ê·¸ ì¤‘ ì–´ë–¤ ì‘ë‹µì„ ì„ í˜¸í•˜ëŠ”ì§€ íƒœê¹…í•©ë‹ˆë‹¤.
3) RL: í•™ìŠµëœ RM ê³¼ RL ì•Œê³ ë¦¬ì¦˜ (PPO) ì„ ì´ìš©í•´ ëª¨ë¸ì„ ê°œì„ í•©ë‹ˆë‹¤.

ì´ ì¤‘ Step 2ì™€ Step 3ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ë””í…Œì¼í•˜ê²Œ ìˆ˜ì‹ì„ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

- Reward model training
  - Bradley-Terry Preference model
    - ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í•™ìŠµëœ Reward í•¨ìˆ˜ $r(x,y)$ ê°€ ìˆì„ ë•Œ $y_1$ì´ $y_2$ ë³´ë‹¤ ì„ í˜¸ë  í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë©ë‹ˆë‹¤. Reward output ì˜ Softmax ë¼ê³  ì´í•´í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ë§í•´, ìƒëŒ€í¸ë³´ë‹¤ reward ê°€ ë†’ìœ¼ë©´ ë†’ì„ ìˆ˜ë¡ ì„ í˜¸ë  í™•ë¥ ì€ ë†’ìŠµë‹ˆë‹¤.
    
    $$
    p^*(y_1 > y_2 | x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}.
    $$
    
  - Reward model loss function 
    - í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‘ê°œì˜ ì‘ë‹µìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¥¼ ë§Œë“¤ê³ , ì‚¬ëŒì´ ì„ í˜¸ë„ë¥¼ íƒœê¹…í•©ë‹ˆë‹¤. ì„ í˜¸ëœ ì‘ë‹µì„ $y_w$, ë°˜ëŒ€ ì‘ë‹µì„ $y_l$ ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    - ì•„ë˜ Loss ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. $y_w$ reward ëŠ” ë†’ì•„ì§€ê³  $y_l$ì˜ reward ëŠ” ë‚®ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµë©ë‹ˆë‹¤.
    
    $$
    \mathcal{L}_R(r_\phi, D) = -\mathbb{E}_{(x,y_w,y_l) \sim D}[\log \sigma(r_\phi (x, y_w) - r_\phi (x, y_l))]
    $$
    
- RL (PPO)
  - PPO ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ì•„ë˜ì˜ Objective function ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
  
    $$
    \max_{\pi_\theta} \mathbb{E}_{x \sim D, y \sim \pi_\theta(y|x)}[r_\phi (x, y)] - \beta D_{KL}[\pi_\theta(y | x) || \pi_{\text{ref}}(y | x)]
    $$
  - Reward ë¥¼ ìµœëŒ€í™” í•˜ë©´ì„œ ê¸°ì¡´ì˜ reference ëª¨ë¸ê³¼ KL-divergence ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
  - KL-divergence ë¥¼ regularizer ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê¸°ì¡´ì— í•™ìŠµëœ ì§€ì‹ë“¤ì„ ë„ˆë¬´ ë§ì´ ìƒì§€ ì•Šë„ë¡ í•˜ëŠ”ë° ìˆìŠµë‹ˆë‹¤. ë˜í•œ Regularizer ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ RM ì— over-optimized ë˜ëŠ” reward hacking í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ```
    ğŸš¨ LLM ì—ì„œ Reward hacking ì´ë€?
    
    â€œë¬´ì—‡ì„ ë„ì™€ë“œë¦¬ë©´ ë ê¹Œìš”?â€, â€œê·¸ë ‡ê²Œ í•œ ë²ˆ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ì§€ ì•Œë ¤ì£¼ì„¸ìš”!â€ ì™€ ê°™ì´ ì“¸ë°ì—†ì§€ë§Œ, ê±°ì§“ë§ì„ í•˜ëŠ” ë‹µë³€ë³´ë‹¤ëŠ” ì„ í˜¸ë  í™•ë¥ ì´ ë†’ì€, ì‚¬ì‹¤ìƒ ë¬´ì˜ë¯¸í•œ ë‹µë³€ë“¤ì„ ê³„ì† ìƒì„±í•´ë²„ë¦¬ëŠ” í˜„ìƒ
    ```
    
í•´ë‹¹ ë°©ë²•ë¡ ì€ íš¨ê³¼ì ì´ì—ˆì§€ë§Œ 3ê°€ì§€ ì¹˜ëª…ì ì¸ ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.  
1) Expensive human annotator 
   - ì‚¬ëŒì´ í‰ê°€í•œ preference ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ë¹„ìš©ì ìœ¼ë¡œë„, ì‹œê°„ì ìœ¼ë¡œë„ êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. 
   - í˜„ì¬ ë§ì€ open-source ë°ì´í„°ê°€ ì¡´ì¬í•˜ê¸°ëŠ” í•˜ì§€ë§Œ, ê³ ë„ë¡œ ì •ì œëœ í•œêµ­ì–´ preference ë°ì´í„°ëŠ” ì°¾ì•„ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ ì˜ì–´ ë²ˆì—­ë³¸ë“¤ì…ë‹ˆë‹¤.
2) Expensive computational cost
   - PPO ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•´ì„œëŠ” 4ê°œì˜ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤: Policy model, Reference model, Reward model, Value model
   - Policy model ë§Œ í•´ë„ 175Bì— ìœ¡ë°•í•˜ëŠ” ìƒí™©ì—ì„œ ì¶”ê°€ë¡œ ë¹„ìŠ·í•œ í¬ê¸°ì˜ 3ê°œ ëª¨ë¸ì„ ë” ë„ìš¸ë§Œí•œ GPU ê³µê°„ì„ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.
  
ì´ ì²« ë²ˆì§¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2023ë…„ Google Research ì—ì„œëŠ” RLAIF ë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## RLAIF

Google Research ì—ì„œ 2023ë…„ì— ë°œí‘œí•œ [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267.pdf) (Lee et al., 2023) ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. 

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/0d05918a-93fe-4a2a-9af2-02f240faf93a)

ì»¨ì…‰ì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. RLHF ì—ì„œ Human feedback ì„ AI feedback ìœ¼ë¡œ ëŒ€ì²´í•˜ê² ë‹¤ëŠ” ê²ƒì¸ë°ìš”. ì´ëŠ” [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) (Zheng et al., 2023) ì—ì„œë„ ì£¼ì¥ë˜ëŠ” ë‚´ìš©ìœ¼ë¡œ, LLM ì´ ì¶©ë¶„íˆ ì‚¬ëŒì˜ í‰ê°€ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒì„ ì´ìš©í•œ ê²ƒì…ë‹ˆë‹¤.

> chatGPT, Bard, Claude ê°™ì€ Off-the-shelve LLM ë“¤ì—ê²Œ í‰ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•´ ë‹µë³€ì˜ ì„ í˜¸ë„ë¥¼ íƒœê¹…í•˜ëŠ” ê²ƒì´ ì‚¬ëŒì´ ì§ì ‘ íƒœê¹…í•˜ëŠ” ê²ƒê³¼ ë†’ì€ correlation ì„ ê°€ì§„ë‹¤
> ![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/0eb2bdb2-763b-4a87-9f40-5e4c3197a61a)

ì‹¤ì œë¡œ RLAIF ë¥¼ ì‚¬ìš©í•  ë•Œì™€ RLHF ë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” í¬ê²Œ ë‚˜ì§€ ì•Šì•˜ì§€ë§Œ

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/e9265359-fc35-4f0c-8050-aa4cd5f9e150)

ê°€ê²© ë©´ì—ì„œëŠ” AI labeler ê°€ 0.06 $/example ì¸ë° ë°˜í•´ Human labelerëŠ” 0.67 $/exampleë¡œ 11ë°°ê°€ ë„˜ê²Œ ì°¨ì´ë‚œë‹¤ê³  ì£¼ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.

ë˜í•œ AI feedback ì€ ê°€ê²© ë¿ë§Œ ì•„ë‹ˆë¼ ì†ë„ì—ë„ ì´ì ì´ ìˆì–´ Iterative training ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
RLHF ì˜ ê²½ìš° í•™ìŠµ ì‚¬ì´í´ì´ ì™„ë£Œëœ í›„ í•™ìŠµëœ ëª¨ë¸ì˜ ì‘ë‹µë“¤ì„ ëª¨ì•„ human annotator ì—ê²Œ ë‹¤ì‹œ íƒœê¹…ì„ ìš”ì²­í•˜ëŠ” ê³¼ì •ì—ì„œ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ RLAIF ëŠ” AI feedback ì„ ì–¸ì œë“  í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í•™ìŠµ ì‚¬ì´í´ì„ ëª‡ ë²ˆì´ê³  ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

## ReST (Deepmind)

Deepmind ì—ì„œëŠ” [Reinforced Self-Training (ReST) for Language Modeling](https://arxiv.org/pdf/2308.08998.pdf) (Gulcehre et al., 2023) ë…¼ë¬¸ì„ í†µí•´ ReST ë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí–ˆëŠ”ë°ìš”, ì´ëŠ” RLHF ë¥¼ iterative í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. 
    
![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/f8178eff-8625-4552-816b-5b84aa7b7462)
    
ReST ëŠ” 4ê°œì˜ step ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. 
1) RM training: HF ë°ì´í„°ë¡œ RM í•™ìŠµ
2) Grow step: Current policy modelì„ ì´ìš©í•´ í•˜ë‚˜ì˜ prompt ì— ëŒ€í•œ ì‘ë‹µ ì—¬ëŸ¬ ê°œ ìƒì„±
3) Data tagging: RM ì„ ì´ìš©í•´ preference íƒœê¹…
   - ì´ ë•Œ reward threshold ë³´ë‹¤ rewardê°€ ë†’ì€ ë°ì´í„°ë§Œ ë‹¤ìŒ stepì— ì‚¬ìš©
4) Improve step: step 3ì—ì„œ ëª¨ì€ ë°ì´í„°ì™€ Offline RL ë°©ë²•ë¡ ì„ ì‚¬ìš©í•´ policy í•™ìŠµ
5) 2-4 step ë°˜ë³µ

ì²˜ìŒ ë§Œë“¤ì–´ ë†“ì€ RM ì„ í™œìš©í•˜ì—¬ ìì²´ ì œì‘í•œ ì‘ë‹µë“¤ì— ëŒ€í•´ì„œë„ preference íƒœê¹…ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ë§Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ iteration ì„ ë°˜ë³µí•  ìˆ˜ë¡ reward threshold ë¥¼ ì ì  ë†’ì—¬ê°€ë©° data quality ë¥¼ í–¥ìƒì‹œì¼œì•¼ ì„±ëŠ¥ì´ ì ì§„ì ìœ¼ë¡œ í–¥ìƒë  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/06e6539d-58e6-4b1d-9067-836bfba2e9e6)

ë˜í•œ ì—¬ëŸ¬ ë²ˆì˜ iteration ì„ ëŒ ë•Œì—ëŠ” learning rate ë¥¼ ì‘ê²Œ ìœ ì§€í•´ì„œ ëª¨ë¸ì´ ì´ì „ì˜ ëª¨ë¸ì—ì„œ ê¸‰ê²©í•˜ê²Œ ë³€í•˜ëŠ” ê²ƒì„ ë§‰ì•„ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤.

ì•„ë˜ ì‹¤í—˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ReST ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ RLHF (Online RL) ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/15718b73-8e91-4dd5-a81d-abeb5b508d8c)


ë‹¤ë§Œ iteration ì„ ë„ˆë¬´ ë§ì´ ë°˜ë³µí•˜ë©´ Reward hacking ì´ ë°œìƒí•œë‹¤ëŠ” ë¦¬í¬íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë˜í”„ì²˜ëŸ¼ average reward ëŠ” ì¦ê°€í–ˆì§€ë§Œ Initial model ê³¼ì˜ Human evaluation score ì°¨ì´ëŠ” ì ì  ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì ì ˆí•œ iteration ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì°¾ì•„ë‚´ëŠ” ì‘ì—…ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.

![image](https://github.com/snulion-study/algorithm-adv/assets/57203764/528b2fa8-95d6-43d7-bcfb-973bc86b83d9)


# References

- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
- [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) (Stiennon et al., 2020)
- [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267.pdf) (Lee et al., 2023)
- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) (Zheng et al., 2023)
- [Reinforced Self-Training (ReST) for Language Modeling](https://arxiv.org/pdf/2308.08998.pdf) (Gulcehre et al., 2023)